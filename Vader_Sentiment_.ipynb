{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vader Sentiment .ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM1wW6rc7Rwt0snV5xdMmXp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anirudh1906/Major_Project/blob/main/Vader_Sentiment_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llghwO1cI1zy",
        "outputId": "1148ed4b-6813-4ae1-a97f-d0e686252bbe"
      },
      "source": [
        "!pip install vaderSentiment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.7/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOctdtwROQSH"
      },
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "vs =SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rwhlfn_Oj_E"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Xyl8JiFfKvl"
      },
      "source": [
        "url= \"http://www.inshorts.com/en/read/sports\"\n",
        "news_data = []\n",
        "news_category=url.split('/')[-1]\n",
        "news_category\n",
        "data=requests.get(url)\n",
        "soup=BeautifulSoup(data.content)\n",
        "#print(soup)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L_XlXR09KOj"
      },
      "source": [
        "urls= ['http://www.inshorts.com/en/read/sports',\n",
        "       'http://www.inshorts.com/en/read/world',\n",
        "       'http://www.inshorts.com/en/read/politics']\n",
        "\n",
        "def build_dataset(urls):  \n",
        "  news_data = []\n",
        "  for url in urls:\n",
        "    news_category=url.split('/')[-1]\n",
        "    data=requests.get(url)\n",
        "    soup=BeautifulSoup(data.content)\n",
        "\n",
        "    news_articles=[{\"news_headline\":headline.find('span',attrs={\"itemprop\":\"headline\"}).string,\n",
        "                  \"news_article\":article.find('div',attrs={\"itemprop\":\"articleBody\"}).string,\n",
        "                  \"news_category\":news_category}\n",
        "                  \n",
        "                  for headline,article in zip(soup.find_all(\"div\",class_=[\"news-card-title news-right-box\"]),\n",
        "                                              soup.find_all(\"div\",class_=[\"news-card-content news-right-box\"]))]\n",
        "  \n",
        "    news_articles=news_articles[0:20]\n",
        "    news_data.extend(news_articles)\n",
        "  \n",
        "  \n",
        "  df=pd.DataFrame(news_data)\n",
        "  df=df[[\"news_headline\",\"news_article\",\"news_category\"]]\n",
        "  return df\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "UEWEKUjzdutZ",
        "outputId": "3a3fda91-cef1-4ea8-d6f4-7c2ac49cd723"
      },
      "source": [
        "df= build_dataset(urls)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>news_headline</th>\n",
              "      <th>news_article</th>\n",
              "      <th>news_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I was abused for saying it's raining, not waki...</td>\n",
              "      <td>Cricketer-commentator Dinesh Karthik has revea...</td>\n",
              "      <td>sports</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bangladesh's Taskin does dance step while batt...</td>\n",
              "      <td>Bangladesh's Taskin Ahmed did a dance step aft...</td>\n",
              "      <td>sports</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Move over Gal Gadot, the real Wonder Woman is ...</td>\n",
              "      <td>India Women's Harleen Deol pulled off a catch ...</td>\n",
              "      <td>sports</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Djokovic defeats Shapovalov in Wimbledon semi-...</td>\n",
              "      <td>World number one Novak Djokovic defeated world...</td>\n",
              "      <td>sports</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pakistan were pathetic: Vaughan after England ...</td>\n",
              "      <td>After England defeated Pakistan by 9 wickets i...</td>\n",
              "      <td>sports</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       news_headline  ... news_category\n",
              "0  I was abused for saying it's raining, not waki...  ...        sports\n",
              "1  Bangladesh's Taskin does dance step while batt...  ...        sports\n",
              "2  Move over Gal Gadot, the real Wonder Woman is ...  ...        sports\n",
              "3  Djokovic defeats Shapovalov in Wimbledon semi-...  ...        sports\n",
              "4  Pakistan were pathetic: Vaughan after England ...  ...        sports\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-ELPmhWwxB7",
        "outputId": "4fbc0b37-d72e-4db0-d63b-4658bce9ceef"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "stopwords_list=nltk.corpus.stopwords.words(\"english\")\n",
        "stopwords_list.remove(\"no\")\n",
        "stopwords_list.remove(\"not\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7s9sMSx2UFsU"
      },
      "source": [
        "def html_tag(text):\n",
        "  soup=BeautifulSoup(text,\"html.parser\")\n",
        "  new_text=soup.get_text()\n",
        "  return new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TB3kSD66cx3C",
        "outputId": "fc3e8dc4-3497-4285-9827-f779ffd1bf66"
      },
      "source": [
        "!pip install contractions\n",
        "import contractions\n",
        "def con(text):\n",
        "  expand=contractions.fix(text)\n",
        "  return expand\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.52)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.2.0)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gugZP_CdLvq"
      },
      "source": [
        "import re\n",
        "def remove_sp(text):\n",
        "   pattern=r\"[^a-zA-Z0-9\\s]\"\n",
        "   text=re.sub(pattern,'',text)\n",
        "   return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20YXlX6LgdXp"
      },
      "source": [
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "tokenizer=ToktokTokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np7BHjiPew7l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f357c2e-03a5-4319-e0ab-875e32b27b12"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  tokens=tokenizer.tokenize(text)\n",
        "  tokens=[token.strip() for token in tokens]\n",
        "  filtered_tokens=[token for token in tokens if token not in stopwords_list]\n",
        "  filtered_text=' '.join(filtered_tokens)\n",
        "  return filtered_text\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14oWPBd3CwMQ"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# lemmatize string\n",
        "def lemmatize_word(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    # provide context i.e. part-of-speech\n",
        "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
        "    return lemmas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b441D63igMCb"
      },
      "source": [
        "df.news_headline=df.news_headline.apply(lambda x:x.lower())\n",
        "df.news_article=df.news_article.apply(lambda x:x.lower())\n",
        "\n",
        "df.news_headline=df.news_headline.apply(html_tag)\n",
        "df.news_article=df.news_article.apply(html_tag)\n",
        "\n",
        "df.news_headline=df.news_headline.apply(con)\n",
        "df.news_article=df.news_article.apply(con)\n",
        "\n",
        "df.news_headline=df.news_headline.apply(remove_sp)\n",
        "df.news_article=df.news_article.apply(remove_sp)\n",
        "\n",
        "df.news_headline=df.news_headline.apply(remove_stopwords)\n",
        "df.news_article=df.news_article.apply(remove_stopwords)\n",
        "\n",
        "df.news_headline=df.news_headline.apply(lemmatize_word)\n",
        "df.news_article=df.news_article.apply(lemmatize_word)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp9omLowTQRv"
      },
      "source": [
        "df[\"compound\"]=df[\"news_article\"].apply(lambda x: vs.polarity_scores(x)[\"compound\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEnyxwWREkEw",
        "outputId": "2206431e-19d9-4db7-a3d8-db1dd0649cf4"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vafrAtBxEGT_",
        "outputId": "0098e9b7-1ba5-4ac0-d5d0-8b5d94262931"
      },
      "source": [
        "for i in range(0,60):\n",
        "  if df[\"compound\"][i]>=0.05:\n",
        "    df[\"compound\"][i]=\"positive\"\n",
        "  elif df[\"compound\"][i]>-0.05 and df[\"compound\"][i]<0.05:\n",
        "    df[\"compound\"][i]=\"neutral\"\n",
        "  elif df[\"compound\"][i]<=-0.05:\n",
        "    df[\"compound\"][i]=\"negative\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "G6X2wxrHYwne",
        "outputId": "d19fab61-0fea-44f7-bcc1-98a5b10baa9e"
      },
      "source": [
        "import joblib\n",
        "joblib.dump(SentimentAnalysis)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-fb0dafc5bfe0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSentimentAnalysis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'SentimentAnalysis' is not defined"
          ]
        }
      ]
    }
  ]
}